{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From concurrent.futures to Dataframes\n",
    "\n",
    "In this notebook we look at real data while using a cluster of computers.  For programming we will start with concurrent.futures and then transition to parallel dataframes.  This will give us experience with real data and provide some intuition about what is happening when we use big dataframes such as are provided by Spark or Dask dataframe.\n",
    "\n",
    "To begin, we look at the [New York City Taxi Cab dataset](http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml).  This includes every ride made in the city of New York in the year 2016.  This data is stored in the Parquet format, which we can read with the [fastparquet](http://fastparquet.readthedocs.io/en/latest/) and [gcsfs](http://gcsfs.readthedocs.io/en/latest/) Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcsfs import GCSFileSystem\n",
    "gcs = GCSFileSystem(token='cloud')\n",
    "gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastparquet\n",
    "\n",
    "pf = fastparquet.ParquetFile('anaconda-public-data/nyc-taxi/2015.parquet', open_with=gcs.open)\n",
    "pf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a subset \n",
    "\n",
    "Normally we would call the `pf.to_pandas()` method to read this data into memory as a Pandas dataframe.  However in this case that would be unwise because this data is too large to fit comfortably in RAM (please do not try this, you will likely kill your notebook session).\n",
    "\n",
    "Fortunately Parquet files are split into row groups, each of which does fit nicely into memory.  The following function will read a single row group for us from our Parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastparquet.api import _pre_allocate\n",
    "from fastparquet.core import read_row_group_file\n",
    "\n",
    "columns = ['tpep_pickup_datetime', 'passenger_count', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'payment_type', 'fare_amount', 'tip_amount', 'total_amount']\n",
    "\n",
    "def read_row_group(rg):\n",
    "    fn = pf.row_group_filename(rg)\n",
    "    categories = {}\n",
    "    index = None\n",
    "    cs = {}\n",
    "    dt = pf.dtypes\n",
    "    schema = pf.schema\n",
    "\n",
    "    df, views = _pre_allocate(rg.num_rows, columns, categories, index, cs, dt)\n",
    "    read_row_group_file(fn,rg, columns, categories, schema, cs,\n",
    "                        open=gcs.open, assign=views)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_row_group(pf.row_groups[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of this function call is one Pandas dataframe with a few million rows.  There are several such row groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pf.row_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remote execution with concurrent.futures\n",
    "\n",
    "While we don't have enough memory to handle all of this data locally, we can ask the machines in our cluster to do this work for us.  We connect to the cluster with Dask below and use the concurrent.futures interface to load call this same function remotely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, progress\n",
    "client = Client('schedulers:9000')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = client.submit(read_row_group, pf.row_groups[0])\n",
    "future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you watch [Dask's diagnostic dashboard](../../../9002/status) you will see this function run and stay in memory on one of the remote workers.\n",
    "\n",
    "The Pandas dataframe now lives on that machine.  We can submit computations to run on that remote dataframe by submitting new tasks onto our future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_future = client.submit(len, future)\n",
    "len_future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This too runs remotely.  By calling submit on futures we can chain computations without ever bringing the data back to our local machine.\n",
    "\n",
    "However, if we do want to bring data back, we can do so with the `.result()` method like before.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_future.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a `ThreadPoolExecutor` calling `.result()` did two things\n",
    "\n",
    "1.  Wait for the computation to finish\n",
    "2.  Return the finished value\n",
    "\n",
    "Now calling result does *three* things\n",
    "\n",
    "1.  Wait for the computation to finish\n",
    "2.  **Communicate** the data from the worker to our local machine\n",
    "3.  Return the finished value\n",
    "\n",
    "This extra step of communication can be *expensive* so we prefer not to call result unless we really have to.  For example, it might take a while if we gather the full dataframe back from the worker to our local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time local_df = future.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This extra cost of communication is something that we should be aware of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Now that we have some real data, lets compute some things about New York.\n",
    "\n",
    "1.  How many passengers rode in cab rides in 2016 total?\n",
    "2.  How many rides had more than two passengers?\n",
    "3.  What was the average number of passengers over all rides?\n",
    "4.  (hard) How many rides were there holding one passenger, two passengers, three passengers, etc..\n",
    "\n",
    "First, use `client.submit` or `client.map` and the `read_row_group` function on each of the row groups to create a list of futures of Pandas dataframes in remote memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use map or submit with the `read_row_group` function \n",
    "# on each of the row groups to get a list of futures of Pandas dataframes\n",
    "\n",
    "futures = ...\n",
    "\n",
    "# How much memory do these take up across the cluster\n",
    "# (this is on the diagnostic dashboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many passengers rode in cab rides in 2016 total?\n",
    "# (answer provided for this question)\n",
    "\n",
    "def f(df):\n",
    "    return df.passenger_count.sum()\n",
    "\n",
    "counts = client.map(f, futures)\n",
    "total = client.submit(sum, counts)\n",
    "total.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many rides were there that had more than two passengers?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What was the average number of passengers over all rides?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (hard) How many rides were there for each passenger count?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load solutions/nyc-futures.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Up Next\n",
    "\n",
    "The exercises that we have just done are exactly how projects like Spark Dataframes and Dask dataframes work and the algorithms that we've built are very similar to the algorithms contained within those projects.  However, because all of these tricks have already been implemented we can use them to accomplish the same results, but in much less code.\n",
    "\n",
    "It's useful to remember that \"big\" dataframes are just collections of smaller in-memory dataframes on which we run normal functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "df = dd.read_parquet('gcs://anaconda-public-data/nyc-taxi/2015.parquet',\n",
    "                     columns=columns,\n",
    "                     storage_options={'token': 'cloud'}).persist()\n",
    "progress(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.passenger_count.sum().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.passenger_count.mean().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.passenger_count.value_counts().compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
