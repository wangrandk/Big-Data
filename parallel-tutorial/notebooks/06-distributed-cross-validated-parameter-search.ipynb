{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributed Cross Validated Parameter Search\n",
    "------------------------------------\n",
    "\n",
    "In the previous section we parallelized cross-validated parameter search on a single machine.  In this notebook we do the same exercise, but now on a distributed cluster.  \n",
    "\n",
    "### Requirements\n",
    "\n",
    "This notebook should be run on the provided cluster.  View the README for connection information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Software Environment\n",
    "\n",
    "`cv_params_demo` is a local .py file that defines the functions we are going to use.\n",
    "In the local case, we imported functions from this module.\n",
    "We will run into issues if our worker machines lack the `cv_params_demo.py` file.\n",
    "Distributed computing frameworks have mechanisms to solve this by sending .py files around.\n",
    "In order to skip dealing with this, we are going to include all of the content of that file in this notebook with the `%run` magic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run cv_params_demo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Collect Data\n",
    "digits = load_digits()  \n",
    "\n",
    "# Construct parameter grid\n",
    "param_grid = {\n",
    "    'C': np.logspace(-10, 10, 1001),\n",
    "    'gamma': np.logspace(-10, 10, 1001),\n",
    "    'tol': np.logspace(-4, -1, 4),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Distributed parallel cross validated parameter search\n",
    "\n",
    "We extend the concurrent.futures and Spark solutions to scale our computation across multiple machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_splits = [load_cv_split(i) for i in range(2)]  # Increase the number 2 after parallel computation acheived\n",
    "param_samples = ParameterSampler(param_grid, 10)    # Increase the number 10 after parallel computation acheived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concurrent.futures solution\n",
    "\n",
    "We've included the `concurrent.futures` solution from the previous notebook below.  You may want to replace the `ThreadPoolExecutor` below with a `dask.distributed.Client` object and point it to the Dask scheduler.\n",
    "\n",
    "While it runs you may want to use [Dask's diagnostic dashboard](../../../9002/status) to get feedback from the cluster.  We recommend setting up the dashboard and your notebook side-by-side on your computer screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load solutions/cvgs-1.py\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "e = ThreadPoolExecutor()\n",
    "\n",
    "futures = []\n",
    "\n",
    "parameters = list(param_samples)\n",
    "\n",
    "for split in cv_splits:\n",
    "    for params in parameters:\n",
    "        future = e.submit(evaluate_one, SVC, params, split)\n",
    "        futures.append(future)\n",
    "\n",
    "results = [f.result() for f in futures]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Solution\n",
    "\n",
    "Here we provide the local Spark solution to this problem as well.  Redirect your SparkContext to the Spark master.  While this runs you may want to use [Spark's diagnostic dashboard](../../../9070) to get feedback from the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(...)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_rdd = sc.parallelize(cv_splits)\n",
    "param_rdd = sc.parallelize(list(param_samples))\n",
    "\n",
    "rdd = param_rdd.cartesian(cv_rdd)\n",
    "results = rdd.map(lambda tup: evaluate_one(SVC, tup[0], tup[1]))\n",
    "\n",
    "results = results.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to terminate your spark context\n",
    "# sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concluding thoughts\n",
    "\n",
    "1.  Scaling computations can give you more precise insight into sampling problems\n",
    "2.  The lessons you learned in the last section carry over from your laptop to cluster computing\n",
    "3.  Visual diagnostic dashboards can help connect you to what is happening on your cluster"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
